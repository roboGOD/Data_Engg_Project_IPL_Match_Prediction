{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPstSUrIXdu2",
        "outputId": "0297d0a4-c2e6-4e07-c8f2-fe4f0032f74d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: apt-get\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4OuJhrr1XkmG",
        "outputId": "1c9cf9bc-4e5f-4079-980a-b378ddb2fd86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: apt-get\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y openjdk-11-jdk-headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2f3wF8VZi86",
        "outputId": "fff34702-a701-4b83-a080-c8f86cd32438"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkYc5rKGStda"
      },
      "source": [
        "**Initialize Spark**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A36Uom17hmIY",
        "outputId": "966cef4a-0430-4d7e-d993-717ff0a15026"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: apt-get\n",
            "zsh:1: command not found: wget\n",
            "tar: Error opening archive: Failed to open 'spark-3.5.1-bin-hadoop3.tgz'\n"
          ]
        }
      ],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar -xvf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qiUsvpVASZVy"
      },
      "outputs": [
        {
          "ename": "Exception",
          "evalue": "Unable to find py4j in /content/spark-3.5.1-bin-hadoop3/python, your SPARK_HOME may not be configured correctly",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     py4j \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spark_python, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy4j-*.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSPARK_HOME\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/spark-3.5.1-bin-hadoop3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m findspark\u001b[38;5;241m.\u001b[39minit()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, when, regexp_extract, split, year, month, dayofmonth, median, \u001b[38;5;28msum\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m Fsum, to_date, create_map, lit, lower, element_at\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m         py4j \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spark_python, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy4j-*.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find py4j in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    163\u001b[0m                 spark_python\n\u001b[1;32m    164\u001b[0m             )\n\u001b[1;32m    165\u001b[0m         )\n\u001b[1;32m    166\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath[:\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m sys_path \u001b[38;5;241m=\u001b[39m [spark_python, py4j]\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# already imported, no need to patch sys.path\u001b[39;00m\n",
            "\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.5.1-bin-hadoop3/python, your SPARK_HOME may not be configured correctly"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, regexp_extract, split, year, month, dayofmonth, median, sum as Fsum, to_date, create_map, lit, lower, element_at\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my1nIyrQV7UA"
      },
      "source": [
        "**FULL CLEANING PIPELINE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rv3w9jqll4a",
        "outputId": "04964237-6001-4f5c-b89b-a5cc7d00b96b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded matches rows: 1095\n",
            "Loaded deliveries rows: 260920\n",
            "Rows in final cleaned df: 1095\n",
            "+--------------------+--------------------+--------------------+-------------+--------------------+----------+------------+----------+----+-----+--------------------+\n",
            "|               team1|               team2|         toss_winner|toss_decision|               venue|      city|season_start|season_end|year|month|              winner|\n",
            "+--------------------+--------------------+--------------------+-------------+--------------------+----------+------------+----------+----+-----+--------------------+\n",
            "|royal challengers...|kolkata knight ri...|royal challengers...|        field|m chinnaswamy sta...| bangalore|        2007|      2008|2008|    4|kolkata knight ri...|\n",
            "|     kings xi punjab| chennai super kings| chennai super kings|          bat|punjab cricket as...|chandigarh|        2007|      2008|2008|    4| chennai super kings|\n",
            "|    delhi daredevils|    rajasthan royals|    rajasthan royals|          bat|    feroz shah kotla|     delhi|        2007|      2008|2008|    4|    delhi daredevils|\n",
            "|      mumbai indians|royal challengers...|      mumbai indians|          bat|    wankhede stadium|    mumbai|        2007|      2008|2008|    4|royal challengers...|\n",
            "|kolkata knight ri...|     deccan chargers|     deccan chargers|          bat|        eden gardens|   kolkata|        2007|      2008|2008|    4|kolkata knight ri...|\n",
            "+--------------------+--------------------+--------------------+-------------+--------------------+----------+------------+----------+----+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Saved cleaned dataset to: /content/drive/MyDrive/ipl_match_dataset/cleaned_ipl_winner_prediction.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# FINAL PySpark Cleaning Notebook for IPL Winner Prediction\n",
        "\n",
        "\n",
        "# Install Spark (only for Colab)\n",
        "!apt-get install -qq openjdk-11-jdk-headless > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# 1) Initialize Spark\n",
        "import findspark\n",
        "findspark.init(\"/content/spark-3.5.1-bin-hadoop3\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"IPL_Winner_Cleaning_Final\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "# 2) Load datasets\n",
        "\n",
        "\n",
        "matches_path = \"/content/drive/MyDrive/ipl_match_dataset/matches.csv\"\n",
        "deliveries_path = \"/content/drive/MyDrive/ipl_match_dataset/deliveries.csv\"\n",
        "\n",
        "matches = spark.read.csv(matches_path, header=True, inferSchema=True)\n",
        "deliveries = spark.read.csv(deliveries_path, header=True, inferSchema=True)\n",
        "\n",
        "print(\"Loaded matches rows:\", matches.count())\n",
        "print(\"Loaded deliveries rows:\", deliveries.count())\n",
        "\n",
        "\n",
        "# 3) Drop rows where 'winner' is NULL\n",
        "\n",
        "matches = matches.dropna(subset=[\"winner\"])\n",
        "\n",
        "\n",
        "# 4) Impute missing player_of_match = \"Unknown\"\n",
        "\n",
        "matches = matches.fillna({\"player_of_match\": \"Unknown\"})\n",
        "\n",
        "\n",
        "# 5) Drop unwanted columns: method, umpire1, umpire2\n",
        "\n",
        "cols_drop = [\"method\", \"umpire1\", \"umpire2\"]\n",
        "for c in cols_drop:\n",
        "    if c in matches.columns:\n",
        "        matches = matches.drop(c)\n",
        "\n",
        "\n",
        "# 6) Median imputation for numeric columns (result_margin, target_runs, target_overs)\n",
        "\n",
        "\n",
        "def fill_median(df, colname):\n",
        "    if colname in df.columns:\n",
        "        try:\n",
        "            med = df.approxQuantile(colname, [0.5], 0.05)[0]\n",
        "            return df.fillna({colname: med})\n",
        "        except:\n",
        "            return df\n",
        "    return df\n",
        "\n",
        "for colname in [\"result_margin\", \"target_runs\", \"target_overs\"]:\n",
        "    matches = fill_median(matches, colname)\n",
        "\n",
        "\n",
        "# 7) Split season into season_start and season_end\n",
        "\n",
        "\n",
        "if \"season\" in matches.columns:\n",
        "    matches = matches.withColumn(\"season_parts\", split(col(\"season\"), \"/\"))\n",
        "    matches = matches.withColumn(\"season_start\", col(\"season_parts\").getItem(0).cast(\"int\"))\n",
        "    matches = matches.withColumn(\n",
        "        \"season_end\",\n",
        "        when(\n",
        "            col(\"season_parts\").getItem(1).isNotNull() & (length(col(\"season_parts\").getItem(1)) == 2),\n",
        "            concat(lit(\"20\"), col(\"season_parts\").getItem(1)).cast(\"int\")\n",
        "        ).when(\n",
        "            col(\"season_parts\").getItem(1).isNotNull(),\n",
        "            col(\"season_parts\").getItem(1).cast(\"int\")\n",
        "        ).otherwise(col(\"season_parts\").getItem(0).cast(\"int\"))\n",
        "    )\n",
        "    matches = matches.drop(\"season_parts\")\n",
        "\n",
        "\n",
        "# 8) Standardize team names\n",
        "\n",
        "\n",
        "# Standardize team names (lowercase already applied)\n",
        "team_name_mapping = {\n",
        "    'delhi daredevils': 'delhi capitals',\n",
        "    'kings xi punjab': 'punjab kings',\n",
        "    'rising pune supergiants': 'pune warriors',\n",
        "    'rising pune supergiant': 'pune warriors',\n",
        "    'gujarat lions': 'gujarat titans',\n",
        "    'deccan chargers': 'sunrisers hyderabad',\n",
        "    'royal challengers bengaluru': 'royal challengers bangalore'\n",
        "}\n",
        "\n",
        "# Create a flat list for Spark create_map\n",
        "mapping_expr = create_map(\n",
        "    [lit(key) for key_value in team_name_mapping.items() for key in key_value]\n",
        ")\n",
        "\n",
        "# Replace team names in these columns\n",
        "team_cols = [\"winner\", \"team1\", \"team2\", \"toss_winner\"]\n",
        "\n",
        "for c in team_cols:\n",
        "    if c in matches.columns:\n",
        "        matches = matches.withColumn(\n",
        "            c,\n",
        "            when(\n",
        "                col(c).isin(list(team_name_mapping.keys())),\n",
        "                element_at(mapping_expr, col(c))\n",
        "            ).otherwise(col(c))\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "# 9) Convert ALL string columns in both datasets to lowercase\n",
        "\n",
        "\n",
        "string_cols_matches = [c for c, t in matches.dtypes if t == 'string']\n",
        "for c in string_cols_matches:\n",
        "    matches = matches.withColumn(c, lower(col(c)))\n",
        "\n",
        "string_cols_deliveries = [c for c, t in deliveries.dtypes if t == 'string']\n",
        "for c in string_cols_deliveries:\n",
        "    deliveries = deliveries.withColumn(c, lower(col(c)))\n",
        "\n",
        "\n",
        "# 10) Drop unwanted columns in deliveries (not needed for ML)\n",
        "\n",
        "\n",
        "deliv_cols_drop = [\"player_dismissed\", \"dismissal_kind\", \"fielder\", \"super_sub\"]\n",
        "for c in deliv_cols_drop:\n",
        "    if c in deliveries.columns:\n",
        "        deliveries = deliveries.drop(c)\n",
        "\n",
        "# 11) Select ML-safe features ONLY (NO leakage columns)\n",
        "\n",
        "\n",
        "leakage_cols = [\"win_by_runs\", \"win_by_wickets\", \"result\", \"result_margin\"]\n",
        "for c in leakage_cols:\n",
        "    if c in matches.columns:\n",
        "        matches = matches.drop(c)\n",
        "\n",
        "# Extract date features if present\n",
        "if \"date\" in matches.columns:\n",
        "    matches = matches.withColumn(\"date\", to_date(col(\"date\")))\n",
        "    matches = matches.withColumn(\"year\", year(\"date\"))\n",
        "    matches = matches.withColumn(\"month\", month(\"date\"))\n",
        "    matches = matches.withColumn(\"day\", dayofmonth(\"date\"))\n",
        "    matches = matches.drop(\"date\")\n",
        "\n",
        "# Keep only features known BEFORE match:\n",
        "final_features = [\n",
        "    \"match_id\", \"team1\", \"team2\", \"toss_winner\", \"toss_decision\",\n",
        "    \"venue\", \"city\", \"season_start\", \"season_end\", \"year\", \"month\",\n",
        "    \"winner\"        # label\n",
        "]\n",
        "\n",
        "final_df = matches.select([c for c in final_features if c in matches.columns])\n",
        "\n",
        "print(\"Rows in final cleaned df:\", final_df.count())\n",
        "final_df.show(5)\n",
        "\n",
        "\n",
        "# 12) Save cleaned dataset to Drive\n",
        "\n",
        "\n",
        "out_path = \"/content/drive/MyDrive/ipl_match_dataset/cleaned_ipl_winner_prediction.csv\"\n",
        "final_df.write.csv(out_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "print(\"Saved cleaned dataset to:\", out_path)\n",
        "\n",
        "\n",
        "# END of the code, which will be input to SparkML pipeline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
