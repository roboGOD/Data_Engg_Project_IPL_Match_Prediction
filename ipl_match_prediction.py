# -*- coding: utf-8 -*-
"""ipl_match_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G5LZt3NCyTf6TWMYoCU56hF2TPBtcz8_
"""

!apt-get update -q

!apt-get install -y openjdk-11-jdk-headless

# Run below commands if running in colab
#!apt-get update -q
#!apt-get install -y openjdk-11-jdk-headless

# ------------------------------------------
# spark_ipl_cleaning.py
# ------------------------------------------

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, trim
import kagglehub
import os
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["PATH"] += ":/usr/lib/jvm/java-11-openjdk-amd64/bin"

# ------------------------------------------
# Step 1: Create Spark Session
# ------------------------------------------
spark = SparkSession.builder \
    .appName("IPL_Data_Cleaning") \
    .getOrCreate()

print("âœ… Spark session started successfully!")

# ------------------------------------------
# Step 2: Download dataset from Kaggle
# ------------------------------------------
dataset_path = kagglehub.dataset_download("patrickb1912/ipl-complete-dataset-20082020")
print(f"ðŸ“¥ Dataset downloaded to: {dataset_path}") # Fixed f-string syntax

# ------------------------------------------
# Step 3: Load CSVs into Spark
# ------------------------------------------
deliveries_path = os.path.join(dataset_path, "deliveries.csv")
matches_path = os.path.join(dataset_path, "matches.csv")

deliveries_df = spark.read.option("header", "true").csv(deliveries_path)
matches_df = spark.read.option("header", "true").csv(matches_path)

print("âœ… Deliveries and Matches CSVs loaded into Spark DataFrames")

# ------------------------------------------
# Step 4: Basic Cleaning Function
# ------------------------------------------

def clean_spark_df(df):
    """
    Clean a Spark DataFrame:
      - Trim whitespace from string columns
      - Drop duplicates
      - Drop completely null rows
      - Standardize column names
    """
    # Drop duplicates and null rows
    df_clean = df.dropDuplicates().na.drop(how="all")

    # Normalize column names (lowercase + underscores)
    for old_col in df_clean.columns:
        new_col = (
            old_col.strip()
            .lower()
            .replace(" ", "_")
            .replace("(", "")
            .replace(")", "")
            .replace("-", "_")
        )
        df_clean = df_clean.withColumnRenamed(old_col, new_col)

    # Trim string columns
    string_cols = [f.name for f in df_clean.schema.fields if f.dataType.simpleString() == "string"]
    for col_name in string_cols:
        df_clean = df_clean.withColumn(col_name, trim(col(col_name)))

    return df_clean

# ------------------------------------------
# Step 5: Apply cleaning to both DataFrames
# ------------------------------------------
deliveries_cleaned = clean_spark_df(deliveries_df)
matches_cleaned = clean_spark_df(matches_df)

# ------------------------------------------
# Step 6: Show sample data
# ------------------------------------------
print("\nâœ… Cleaned Deliveries Data:")
deliveries_cleaned.show(5, truncate=False)

print("\nâœ… Cleaned Matches Data:")
matches_cleaned.show(5, truncate=False)

print(f"Deliveries count: {deliveries_cleaned.count()} rows")
print(f"Matches count: {matches_cleaned.count()} rows")

# ------------------------------------------
# Step 7: Save cleaned data for downstream (optional)
# ------------------------------------------
output_dir = "./cleaned_data"
os.makedirs(output_dir, exist_ok=True)

deliveries_cleaned.write.mode("overwrite").option("header", "true").csv(f"{output_dir}/deliveries_cleaned")
matches_cleaned.write.mode("overwrite").option("header", "true").csv(f"{output_dir}/matches_cleaned")

print(f"ðŸ’¾ Cleaned CSVs saved under: {output_dir}/")

# ------------------------------------------
# Step 8: Stop Spark
# ------------------------------------------
spark.stop()
print("ðŸ§¹ Spark job completed successfully!")

